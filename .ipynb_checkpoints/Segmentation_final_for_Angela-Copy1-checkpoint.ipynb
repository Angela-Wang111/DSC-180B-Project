{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e974bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina' # For high-resolution.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydicom as dicom\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import torch \n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "# from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import gc\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b994c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe66481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "USERNAME = os.getcwd().split('/')[2]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "RESOLUTION = 512\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "DROP_LAST = True\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "THRESHOLD = 0.3\n",
    "# MIN_ACTIVATION_SIZE = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c0b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(df_type):\n",
    "    #username = os.getcwd().split('/')[2]\n",
    "    df_path = '/home/{}/teams/dsc-180a---a14-[88137]/CANDID_PTX_csv/{}.csv'.format(USERNAME, df_type)\n",
    "    df = pd.read_csv(df_path)[['Mask_Path', 'XRay_Path', 'Intermediate_Predicted_Path']]\n",
    "#     df = pd.read_csv(df_path)[['Mask_Path', 'XRay_Path']]\n",
    "    df['Mask_Path'] = df['Mask_Path'].str.replace('anw008', USERNAME)\n",
    "    df['XRay_Path'] = df['XRay_Path'].str.replace('anw008', USERNAME)\n",
    "    df['Intermediate_Predicted_Path'] = df['Intermediate_Predicted_Path'].str.replace('anw008', USERNAME)\n",
    "    df['SOP'] = df['XRay_Path'].apply(lambda x: x.split('/')[-1])\n",
    "    \n",
    "    df['No_Pneumothorax'] = df['Mask_Path'].str.contains('negative_mask').astype(int)\n",
    "    df['Yes_Pneumothorax'] = 1 - df['No_Pneumothorax']\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1faae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = read_df('train')\n",
    "val_df = read_df('validation')\n",
    "test_df = read_df('test')\n",
    "pos_df = read_df('train_pos')\n",
    "neg_df = read_df('train_neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487cc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANDID_PTX(Dataset):\n",
    "    def __init__(self, df, resolution, model_type):\n",
    "        self.img_paths = df['XRay_Path'].values\n",
    "        self.intermediate_paths = df['Intermediate_Predicted_Path'].values\n",
    "        self.mask_paths = df['Mask_Path'].values\n",
    "        self.labels = torch.tensor(df[['No_Pneumothorax', 'Yes_Pneumothorax']].values, dtype=torch.float32)\n",
    "        # Just changed by Angela\n",
    "        self.sop = df['SOP'].values\n",
    "        self.resolution = resolution\n",
    "        \n",
    "        # model_type: 'C' for Classification, 'S' for Segmentation, 'E' for Ensemble\n",
    "        self.model_type = model_type\n",
    "              \n",
    "        return\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.img_paths.shape[0]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.model_type == 'E':\n",
    "            # Designed for ensemble model's classification part\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            new_img_path = self.intermediate_paths[idx]\n",
    "            new_img = plt.imread(new_img_path)[:, :, :3]\n",
    "            to_tensor = transforms.ToTensor()\n",
    "            new_img = to_tensor(new_img)\n",
    "            \n",
    "            return new_img, label\n",
    "        \n",
    "        else:\n",
    "            img_path = self.img_paths[idx]\n",
    "            img = dicom.dcmread(img_path).pixel_array\n",
    "            img_min = np.min(img)\n",
    "            img_max = np.max(img)\n",
    "            img_norm = (img - img_min) / (img_max - img_min)\n",
    "            img_norm = cv2.resize(img_norm, (self.resolution, self.resolution))\n",
    "            img_norm = torch.tensor(img_norm).expand(3, self.resolution, self.resolution)\n",
    "\n",
    "            if self.model_type == 'C':\n",
    "                # Designed for classification model\n",
    "                label = self.labels[idx]\n",
    "\n",
    "                return img_norm, label\n",
    "\n",
    "            elif self.model_type == 'S':\n",
    "                # Designed for segmentaion models (might change later)\n",
    "                mask_path = self.mask_paths[idx]\n",
    "                mask = plt.imread(mask_path)[:, :, 0]\n",
    "                mask = cv2.resize(mask, (self.resolution, self.resolution))\n",
    "    #             mask = np.where(mask < 0.5, 0, 1)\n",
    "                mask = torch.tensor(mask).expand(1, self.resolution, self.resolution) \n",
    "        \n",
    "                sop = self.sop[idx]\n",
    "\n",
    "                return img_norm, mask, sop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c04d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(model_type):\n",
    "    train_ds = CANDID_PTX(train_df, RESOLUTION, model_type)\n",
    "    val_ds = CANDID_PTX(val_df, RESOLUTION, model_type)\n",
    "    test_ds = CANDID_PTX(test_df, RESOLUTION, model_type)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, \n",
    "                          pin_memory = PIN_MEMORY, drop_last = DROP_LAST, shuffle = True)\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, \n",
    "                              pin_memory = PIN_MEMORY, drop_last = DROP_LAST, shuffle = False)\n",
    "\n",
    "    test_loader = DataLoader(test_ds, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, \n",
    "                              pin_memory = PIN_MEMORY, drop_last = DROP_LAST, shuffle = False)\n",
    "    \n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6146e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_loaders(schedule_type, cur_df=None, num_neg=0, model_type=None):\n",
    "    if schedule_type == 3:\n",
    "        cur_df = pd.concat([pos_df, neg_df.sample(n=num_neg, replace=False)]).sample(frac=1, ignore_index=True)\n",
    "        \n",
    "    train_ds = CANDID_PTX(cur_df, RESOLUTION, model_type)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, \n",
    "                          pin_memory = PIN_MEMORY, drop_last = DROP_LAST, shuffle = True)\n",
    "\n",
    "    return train_loader, cur_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0901ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE_weight(num_pos, num_neg):\n",
    "    return torch.tensor([1, num_neg / num_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2372426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both_loss(all_train_loss, all_val_loss, model_type, model_name, model_schedule=None):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    epoch_num = len(all_train_loss)\n",
    "    df = pd.DataFrame({'x':np.arange(1, epoch_num+1),\n",
    "                    'train_loss':all_train_loss,\n",
    "                      'val_loss':all_val_loss})\n",
    "    df = df.set_index('x')\n",
    "    train_val_loss = sns.lineplot(data=df, linewidth=2.5)\n",
    "#     train_val_loss.set_xticks(np.arange(1, epoch_num+1, 4), labels=np.arange(1, epoch_num+1, 4))\n",
    "    # set the ticks first\n",
    "    train_val_loss.set_xticks(np.arange(1, epoch_num+1, 1))\n",
    "\n",
    "    # set the labels\n",
    "    train_val_loss.set_xticklabels(np.arange(1, epoch_num+1, 1))\n",
    "\n",
    "\n",
    "    ## now label the y- and x-axes.\n",
    "    plt.ylabel('BCE Loss')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    title = 'BCE Loss of {} model, {}'.format(model_type, model_name)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = train_val_loss.get_figure()\n",
    "    fig.savefig('metric_imgs/{}_type{}.png'.format(title, model_schedule), dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a62194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_true, model_type, model_name, model_schedule=None):\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_test)\n",
    "\n",
    "    cm = sns.heatmap(cm, annot=True, cmap = 'Blues', fmt=\"d\")\n",
    "    title = 'Confusion matrix of {} model, {}'.format(model_type, model_name)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    fig = cm.get_figure()\n",
    "    fig.savefig('metric_imgs/{}_type{}.png'.format(title, model_schedule), dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dc6eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_test, y_true, model_type, model_name, model_schedule=None):\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_test, drop_intermediate = False)\n",
    "    roc_auc = roc_auc_score(y_true, y_test)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1])\n",
    "    roc_plt = plt.plot(fpr, tpr, label='{}(area = {:.3f})'.format(model_name, roc_auc))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    title = 'ROC curve of {} model, {}'.format(model_type, model_name)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('metric_imgs/{}_type{}.png'.format(title, model_schedule), dpi=400)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "231ccb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(test_loader, model, model_type, model_name, model_schedule=None):\n",
    "    \"\"\"\n",
    "    Calculate confusion matrix & auc-roc\n",
    "    Return a list \n",
    "    \"\"\"\n",
    "    y_test = np.array([])\n",
    "    y_true = np.array([])\n",
    "    y_test_prob = np.array([])\n",
    "    total_num_batch = 0\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        total_num_batch += 1\n",
    "        imgs, labels = imgs.to(DEVICE, dtype=torch.float), labels.to(DEVICE, dtype=torch.float)\n",
    "        preds = model(imgs)\n",
    "        \n",
    "        soft_max = nn.Softmax(dim=1)\n",
    "        pred_prob = soft_max(preds).detach().cpu().numpy()\n",
    "        \n",
    "        pred_label = np.argmax(pred_prob, axis=1)\n",
    "        true_label = labels.detach().cpu().numpy().astype(int)[:, 1]\n",
    "        \n",
    "        y_test = np.concatenate((y_test, pred_label))\n",
    "        y_true = np.concatenate((y_true, true_label))\n",
    "\n",
    "        # add this line\n",
    "        y_test_prob = np.concatenate((y_test_prob, pred_prob[:, 1]))\n",
    "    \n",
    "    \n",
    "    plot_confusion_matrix(y_test, y_true, model_type, model_name, model_schedule)\n",
    "    print(\"The F1-Score is: {}\".format(f1_score(y_true, y_test)))\n",
    "    print(\"The Recall (Sensitivity) is: {}\".format(recall_score(y_true, y_test)))\n",
    "    # modify this line, change from y_test to y_test_prob\n",
    "    plot_roc_curve(y_test_prob, y_true, model_type, model_name, model_schedule)\n",
    "    \n",
    "    print('Total Number of Batch Size: ', total_num_batch)\n",
    "    return y_test, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a62d429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_mask(logit_mask, threshold):\n",
    "    mask = np.where(logit_mask <= threshold, 0, 1)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6fdcfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dc(preds, true_mask):\n",
    "    \"\"\"\n",
    "    Helper function to calculate dice coefficient from predicted and true masks, both in binary.\n",
    "    \"\"\"\n",
    "    if np.sum(true_mask) == 0:\n",
    "        # Return 0 if prediction is positive but true mask is negative\n",
    "        if np.sum(preds) != 0:\n",
    "            return 0\n",
    "        else:\n",
    "            # Return 1 if prediction and true mask are both negative\n",
    "            return 1\n",
    "    else:\n",
    "        # Return 0 if prediction is negative but true mask is positive\n",
    "        if np.sum(preds) == 0:\n",
    "            return 0\n",
    "        # Actually calculate dc if both prediction and mask are both positive\n",
    "        dc = np.sum(preds[true_mask==1])*2.0 / (np.sum(preds) + np.sum(true_mask))\n",
    "        return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b200ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_class_2(model, num_epochs, batch_size, learning_rate, \n",
    "                    train_loader, val_loader):\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    all_train_loss = []\n",
    "    all_val_loss = []\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    pos_total = pos_df.shape[0]\n",
    "    neg_total = neg_df.shape[0]\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_train_loss = 0\n",
    "        batch_num = 0\n",
    "        model.train()\n",
    "        \n",
    "        if epoch % 4 == 0:\n",
    "            cur_group = epoch // 4\n",
    "            cur_df = pd.concat([pos_df, \n",
    "                        neg_df.iloc[int(cur_group * pos_total) : \n",
    "                                    int(np.minimum((cur_group + 1) * pos_total, neg_total))]]).sample(frac=1, \n",
    "                                                                                              ignore_index=True)\n",
    "            train_loader, cur_num = create_train_loaders(2, cur_df, num_neg=0, model_type='C')\n",
    "            print('current number of training set is: {}'.format(cur_num))\n",
    "\n",
    "        \n",
    "        for i, (imgs, labels) in enumerate(train_loader):\n",
    "            batch_num += 1\n",
    "            \n",
    "            imgs, labels = imgs.to(DEVICE, dtype=torch.float), labels.to(DEVICE, dtype=torch.float)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            \n",
    "            \n",
    "            if (i == 0) & (epoch == 0):\n",
    "                print(preds)\n",
    "                print(labels)\n",
    "\n",
    "            \n",
    "            loss = loss_fn(preds, labels)\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += float(loss)\n",
    "            \n",
    "            \n",
    "        if epoch % 3 == 0:\n",
    "            print(\"Total # of training batch: \", i + 1)\n",
    "\n",
    "        all_train_loss.append(total_train_loss / batch_num)\n",
    "            \n",
    "            \n",
    "    ## validation set\n",
    "        batch_num = 0\n",
    "        total_val_loss = 0\n",
    "        model.eval()\n",
    "        for i, (imgs, labels) in enumerate(val_loader):\n",
    "            batch_num += 1\n",
    "            \n",
    "            imgs, labels = imgs.to(DEVICE, dtype=torch.float), labels.to(DEVICE, dtype=torch.float)\n",
    "            \n",
    "            preds = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(preds, labels) # is this mean or sum?\n",
    "\n",
    "            total_val_loss += float(loss) # accumulate the total loss for this epoch.\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(\"Total # of validation batch: \", i + 1)\n",
    "\n",
    "        all_val_loss.append(total_val_loss / batch_num)\n",
    "        \n",
    "    \n",
    "    #plot_both_loss(all_train_loss, all_val_loss)\n",
    "        \n",
    "    return model, all_train_loss, all_val_loss#, all_preds, all_masks, all_xray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_seg(model, num_epochs, batch_size, learning_rate, \n",
    "                    train_loader, val_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Main training function to train the first part of the ensemble model, which is the segmentation model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    all_train_loss = []\n",
    "    all_val_loss = []\n",
    "    all_preds = []\n",
    "    all_masks = []\n",
    "    all_xray = []\n",
    "    val_dice = []\n",
    "    all_dice = [] # Validation dice coefficient for the current validation set (one number per epoch, could see trend)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_train_loss = 0\n",
    "        batch_num = 0\n",
    "        if epoch == num_epochs:\n",
    "        # Save the output from the trained segmentation model on training set images once the indicated number of epochs are run\n",
    "            model.eval()\n",
    "            save_images_predicted_by_static_model(model, train_loader, BATCH_SIZE)\n",
    "            save_images_predicted_by_static_model(model, val_loader, BATCH_SIZE)\n",
    "            save_images_predicted_by_static_model(model, test_loader, BATCH_SIZE)\n",
    "            print(\"Saved all the predicted masks!\")\n",
    "                \n",
    "        else:\n",
    "        # If indicated number of epochs are not met, then keep optimizing the segmentation order.\n",
    "            model.train()\n",
    "        \n",
    "            for i, (imgs, masks, sops) in enumerate(train_loader):\n",
    "                batch_num += 1\n",
    "                imgs, masks = imgs.to(DEVICE, dtype=torch.float), masks.to(DEVICE, dtype=torch.float)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(imgs)\n",
    "\n",
    "                # Print values to check dimensions of predicted images\n",
    "                if (i == 0) & (epoch == 0):\n",
    "                    print(preds)\n",
    "                    print(preds.shape)\n",
    "                    print('cuda device_count: {}'.format(torch.cuda.device_count()))\n",
    "                # Calculate loss and do back-propagation, then calculate total loss for this epoch\n",
    "                loss = loss_fn(preds, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss += float(loss.detach().cpu())\n",
    "\n",
    "            all_train_loss.append(total_train_loss / batch_num)\n",
    "            # Print the number of training batch\n",
    "            if epoch == 0:\n",
    "                print(\"Total # of training batch: \", i + 1)\n",
    "            \n",
    "        ## validation set\n",
    "            batch_num = 0\n",
    "            total_val_loss = 0\n",
    "            model.eval()\n",
    "\n",
    "            for i, (imgs, masks, sops) in enumerate(val_loader):\n",
    "                batch_num += 1\n",
    "                # Send imgs and masks to GPU so that they can be input to the model\n",
    "                imgs, masks = imgs.to(DEVICE, dtype=torch.float), masks.to(DEVICE, dtype=torch.float)\n",
    "                if (i == 0) & (epoch == 0):\n",
    "                    print(masks[0].unique())\n",
    "\n",
    "                preds = model(imgs)\n",
    "                loss = loss_fn(preds, masks) # is this mean or sum?\n",
    "                total_val_loss += float(loss.detach().cpu()) # accumulate the total loss for this epoch.\n",
    "\n",
    "                # Calculate validation set dice coefficient change over time \n",
    "                pred_bi = bi_mask(preds.detach().cpu().squeeze(), THRESHOLD)\n",
    "                test_dice = []\n",
    "                for batch_i in range(BATCH_SIZE):\n",
    "                    cur_dc = calculate_dc(pred_bi[batch_i], masks[batch_i].detach().cpu().squeeze().numpy())\n",
    "                    test_dice.append(cur_dc)\n",
    "                val_dice.append(np.mean(test_dice))\n",
    "\n",
    "                # Save the second validation loader set (4 samples) into a variable to be printed if we wish\n",
    "                if i == 1:\n",
    "                    all_preds.append(preds.detach().cpu())\n",
    "                    all_masks.append(masks.detach().cpu())\n",
    "                    all_xray.append(imgs.detach().cpu())\n",
    "            # Calculate the overall validation loss and dice-coefficient\n",
    "            all_val_loss.append(total_val_loss / batch_num)\n",
    "            all_dice.append(np.mean(val_dice))\n",
    "            \n",
    "            if epoch == 0:\n",
    "                print(\"Total # of validation batch: \", i + 1)\n",
    "        \n",
    "    \n",
    "    #plot_both_loss(all_train_loss, all_val_loss)\n",
    "        \n",
    "    return model, all_train_loss, all_val_loss, all_preds, all_masks, all_xray, all_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669f2711",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_180/3420692974.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaders_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'create_loaders' is not defined"
     ]
    }
   ],
   "source": [
    "loaders_seg = create_loaders('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f513c0",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ab2a0",
   "metadata": {},
   "source": [
    "## ResNet34 + U-Net (T 0.5, Min. 375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647241cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seg = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels = 3, classes=1, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea27ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model, train_loss, val_loss, logit_mask, true_mask, ori_xray, val_dice_arr = training_seg(model = model_seg,\n",
    "                                            num_epochs = NUM_EPOCHS, batch_size = BATCH_SIZE,\n",
    "                                            learning_rate = LEARNING_RATE,\n",
    "                                            train_loader = None, \n",
    "                                            val_loader = loaders_seg[1], \n",
    "                                            test_loader = loaders_seg[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722af22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_both_loss(train_loss, val_loss, model_type='Classification', model_name='ResNet 34', model_schedule='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, y_true = test_metrics(test_loader=loaders_class[2], model=resnet_model, \n",
    "                              model_type='Classification', model_name='ResNet 34', model_schedule='2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8349ca0",
   "metadata": {},
   "source": [
    "## ResNet34 + U-Net (T 0.3, Min. 375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seg = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels = 3, classes=1, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d402e87",
   "metadata": {},
   "source": [
    "## EfficientNet-B3 + U-Net (T 0.3, Min. 375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seg_2 = smp.Unet(\"efficientnet-b3\", encoder_weights=\"imagenet\", in_channels = 3, classes=1, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa00a8",
   "metadata": {},
   "source": [
    "## EfficientNet-B3 + FPN (T 0.5, Min. 375)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seg_3 = smp.FPN(\"efficientnet-b3\", encoder_weights=\"imagenet\", in_channels = 3, classes=1, activation=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
